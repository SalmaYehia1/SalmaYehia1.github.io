<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Word Embeddings</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }

        .question {
            margin: 20px 0;
        }
    </style>
</head>

<body>

    <h1>From Words to Vectors: The Journey of Word Embeddings</h1>
    <p>Have you ever wondered how AI models like Chat GPT and Gemini see words and sentences? Well, just like everything
        with AI, it’s all numbers! Words and sentences are converted to “vectors” before being passed to AI models.
        Vectors, if you’re not familiar with them, are just a list of numbers. As we shall see in this post, current AI
        models represent each word in human language by a unique list of numbers, but where it gets REALLY interesting
        is that the AI learns those numbers for every word completely on its own. You just have to give it a ton of
        data, and by data, we’re talking about a large corpus of human text, such as all of Wikipedia for example. </p>
    <h2>Types of Embeddings</h2>
    <p>This blog post will mostly focus on Word and Document embedding. However, the idea of embeddings applies to so
        much more. Embeddings, in general, are a way to convert some object, such as a word, sentence, or even an image
        into a vector. There are many types of embeddings and different approaches for each. This list is only a small
        example of the many kinds of embeddings that exist out there: </p>
    <ul>
        <li><strong>Word Embeddings</strong><br>
            Numerical representations of words capture semantic relationships, allowing computers to understand and
            process language.
        </li>
        <br>
        <li><strong>Document/Sentence Embeddings</strong><br>
            Document/sentence embeddings extend the concept of word embeddings to represent entire documents or
            sentences as single vectors. These embeddings capture the overall meaning and context of the text. These
            embeddings help in tasks like semantic search, document clustering, and text summarization.
        </li>
        <br>
        <li><strong>Image Embeddings</strong><br>
            Image embeddings are numerical representations of images, mapping them to dense vectors in a
            high-dimensional space. These vectors capture an image's features, such as color, texture, and shape.
        </li>
        <br>
        <li><strong>Graph Embeddings</strong><br>
            Graph embeddings are numerical representations of nodes in a graph, mapping them to dense vectors in a
            low-dimensional space. These vectors capture the structural and semantic relationships between nodes in the
            graph.
        </li>
    </ul>
    <h2>What are Word Embeddings?</h2>
    <p>
        Well, word embeddings mean converting a word into a vector. To be more Mathematically precise, we’re mapping, or
        “embedding”, each word to some vector in a d-dimensional vector space, where d is the dimension of the vector
        and is the same for all words. For example, if each word gets mapped to a list of 300 numbers, our word
        embeddings convert each word to a vector in a 300-dimensional vector space. But how exactly do those mappings
        work? Well, let’s dive deeper!

    </p>
    <h3>How Word Embedding Works</h3>
    <p>
        Word embedding begins by training a model on a large text dataset, like Wikipedia. First, the text is
        preprocessed to ensure the quality. This involves splitting the text into individual words, a step known as
        tokenization, and removing unnecessary elements such as stop words, punctuation, and special characters.
    </p>
    <p>
        During training, the model learns to assign a vector to each word, placing words with similar meanings closer
        together in a shared vector space. By this, word embeddings capture both the unique meaning of each word and the
        relationships between them, reflecting how they are used. This highlights the strength of word embeddings in
        positioning words that share meanings or contexts near each other, making it easier to understand the
        relationships between concepts.
    </p>
    <h3>
        Approaches to word embeddings
    </h3>
    <h4>1. Frequency-based Approaches</h4>
    <p>These approaches depend on the occurrences and the word frequency to build vector representations, such as:</p>
    <ol>
        <li><strong>One Hot Encoding</strong></li>
        <li><strong>Bag of Words (BoW)</strong></li>
        <li><strong>Co-occurrence Counts</strong></li>
        <li><strong>Term Frequency-Inverse Document Frequency (TF-IDF)</strong></li>
    </ol>

    <h4>2. Prediction-based Approaches</h4>
    <p>These approaches learn word embeddings by predicting word occurrences based on their context, such as:</p>
    <ol>
        <li><strong>Word2Vec</strong></li>
        <li><strong>GloVe (Global Vectors for Word Representation)</strong></li>
        <li><strong>BERT</strong></li>
    </ol>

    <h3>
        One Hot Encoding
    </h3>

    <p>
        This is the most basic technique to embed words, but as we shall see, it has many disadvantages. To understand
        this technique more concretely, let’s assume we will choose to set the vocabulary, that is all the words our AI
        model will ever know or ever have to deal with, to be the 50,000 most common words in the English language. One
        idea is to sort the words by any order and assign each word a number between 1 and our vocab size, which is
        50,000 in this case. Notice how we’ve already converted each word into a number? The next step is to represent
        each word by a list of numbers of length 50,000 that is all 0’s with a single 1 at the number associated with
        our target word. In the figure below, the words are sorted alphabetically, and the word Apple is assigned the
        number 1, for example. The vector associated with Apple is equal to 1 only at the first index with all the other
        values equal to 0 elsewhere. On the other hand, the last word in our vocab, Zoo, has a corresponding vector that
        is only equal to 1 at the last index and equal to 0 elsewhere.
    </p>


    <figure>
        <img src="Figures/OneHotFigure.jpg" alt="One-hot encoding example"
            style="width: 700px; max-width:700px; margin-top:20px;">
    </figure>

    <p> But why are hot encodings a bad idea? Well, there are a couple of problems with this approach:</p>
    <ol>
        <li>The Vector Dimension is too large = Vocab Size.<br></li>
        <li>The Vectors are sparse, “mostly empty”. Even though we’re using many dimensions per word, we’re
            wasting most of
            our vector dimensions by setting them to 0. We’re only making use of 1 single dimension per word when we
            set it
            equal to 1.<br></li>
        <li> All words are equally distant in our vector space. As we shall see later on, other techniques for
            word
            embeddings have an interesting property in that words with similar meanings get embedded by vectors that
            are
            closer to each other.<br></li>
    </ol>


    <h3>
        Bag Of Words (BOW)
    </h3>
    <p>
        A very similar technique to one-hot encoding used to embed a document, instead of a single word, is known as Bag
        of Words. When embedding a document with a bag of words, we’re essentially getting the one-hot encoding of every
        word in the document and summing them all into one single vector representing the document, thus the text is
        defined as the frequency of words in a document while ignoring the order. The next figure shows how a bag of
        words would work for 3 sentences, each representing a different document, and a vocab size = 4:
    </p>
    <figure>
        <img src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*axffCQ9ae0FHXxhuy66FbA.png"
            alt="Bag of Words Model" width="600">
        <figcaption>Source: <a href="https://ayselaydin.medium.com/4-bag-of-words-model-in-nlp-434cb38cdd1b">4 — Bag of
                Words Model in NLP</a></figcaption>
    </figure>

    <p>

        It provides a more compact representation than one-hot encoding but still lacks context and semantic meaning.
        Each position in the vector corresponds to a word in the vocabulary, in the order they were listed.

    </p>
     <b>Disadvantages:</b>
    <ol>
        <li>
            Similar to one-hot encoding, the vector dimension is still too large = Vocabulary Size.
        </li>
        <li>
            Similar to one-hot encoding, the Bag of Words will still be mostly empty. While the vector in Bag of Words
            wouldn’t be as empty as with One-Hot-Encoded words, it will still have many unused dimensions in our vector
            set to 0. For a bag of words to ever have a vector that isn’t mostly empty, it needs to have a TON of words.
            For example, if we want more than half of our vector to not be set to 0, then we need the encoded document
            to have half of ALL the words in our vocabulary. For a vocab size = 50,000, we would need our document to
            have at least 25,000 unique words. However, this will usually not be the case for any document we’re dealing
            with, so we will end up with mostly empty vectors.
        </li>
        <li>
            Completely ignores the order. While this disadvantage is related to the problem of embedding documents
            rather than embedding a single word, it is an important problem that appears whenever we try to embed a
            document as the sum of embeddings for the words in that document.
        </li>
    </ol>
    <h3>
        Co-occurrence counts
    </h3>
    <p>
        Another way to embed words is by using a co-occurrence matrix. By measuring how often each word occurs with
        other words we can find a suitable representation of those words. In the next figure, for example, we can see
        that the word “digital” occurs with “computer” 1670 times, which is way more than “cherry” and “computer” for
        example. We can use this technique to embed the words by the counts in their corresponding rows.
    </p>

    <h3>
        Term Frequency-Inverse Document Frequency (TF-IDF)
    </h3>
    <p>
        TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection
        of documents. It's commonly used in information retrieval and text mining to help rank the relevance of
        documents based on keyword searches.

        It works by essentially measuring, for every word, how much more commonly it occurs in the specific document we
        are embedding relative to other documents in our data.

        A term like the word “the”, which could appear a lot in our document but also appears in every other document
        does not mean much. However, a scientific article with the term “DNA” that frequently appears in that article,
        but does not frequently appear in general within other documents in our text will likely be a much more powerful
        measure of the meaning of our document.<br><br>

        <b>Disadvantages:</b>
    <ol>
        <li>
            Lack of semantic understanding as it treats words as individual units ignoring their meaning and it doesn't
            understand the meaning of words in different contexts.<br><br>
        </li>
        <li>
            Ignoring word order results in the loss of important contextual information.<br><br>

        </li>
        <li>
            TF-IDF vectors can be very sparse like One Hot Encoding, especially for large vocabularies, leading to
            inefficiencies in storage and computation.<br><br>
        </li>
        <li>
            Synonymous words are treated as distinct, even though they have the same meaning, leading to redundancy and
            lower-quality features.<br><br>
        </li>
        <li>
            It gives weight to words based on how often they appear in a document and how rare they are. But it
            struggles with stopwords and common words like “the,” “and,” or “is” because these words show up a lot but
            don’t add much meaning so this makes it harder for models to find the important patterns in data.<br><br>
        </li>

    </ol>

    </p>

    <h3>
        Mathematics Behind Word Embeddings
    </h3>

    <p>
        To understand word embeddings better, let’s look at them in a more indepth way. When creating word embeddings,
        we want to position similar words close to each other in a high-dimensional space, which helps in understanding
        their meanings and relationships. The main method used to measure this notion of “closeness” in a high
        dimensional space is the <b>"cosine similarity"</b>, which calculates the direction and magnitude of word
        vectors.
        To
        find cosine similarity, we calculate the dot product of two vectors, the sum of the products of their
        corresponding components, and then normalize by their norms. A higher dot product means stronger relationships
        between the words.

        In the following figure for example, ideally, we want the angle between our learned embedding vectors for Apple
        and Orange to be smaller than the angle between Apple and Chair, thus the cosine similarity should be higher in
        the case of Apple and Orange than in Apple and Chair.

    </p>

    <figure>
        <img src="Figures/CosineSimilarity.png" alt="Cosine Similarity" style="width: 1000px; margin-top:20px;">
    </figure>

    <h2>
        Word2Vec
    </h2>

    <p>
        Word2Vec is a technique for converting words into dense vectors. It aims to represent words in a continuous
        vector space, where semantically similar words are mapped to nearby points. These vectors capture information
        about the words based on the words around them.
        Word2Vec uses two main approaches:
    </p>

    <h3>1. Continuous Bag of Words (CBOW):</h3>
    <h3>2. Skip-Gram:</h3>
    <h3>3. Negative Sampling:</h3>

    <h2>
        GloVe (Global Vectors for Word Representation)
    </h2>

    <h2>
        BERT (Bidirectional Encoder Representations from Transformers)
    </h2>


    <h2>
        Conclusion:
    </h2>
    <p>
        In conclusion, word embeddings have revolutionized the way machines interpret human language, forming the
        backbone of modern Natural Language Processing (NLP) applications. Starting from the simplicity of one-hot
        encoding, where each word was represented in isolation, to the bag-of-words model that incorporated word
        frequency, early approaches provided foundational but limited insights into text meaning. The advent of models
        like Word2Vec and GloVe marked a turning point by embedding words in continuous vector spaces, allowing semantic
        similarities and relationships to be captured. This shift enabled models to understand context more effectively,
        improving tasks like sentiment analysis and machine translation. Finally, transformer-based models like BERT
        have pushed word embeddings further, representing words in a deeply contextualized manner by considering their
        surrounding text. This evolution not only captures nuanced meanings but also empowers NLP systems to handle
        complex language tasks with human-like proficiency. As NLP advances, word embeddings will continue to play a
        critical role, bridging the gap between human language and machine understanding.
    </p>


    <h2>
        Future of Word Embeddings:
    </h2>
    <p>
        The future of word embeddings in artificial intelligence is incredibly promising. We have moved from traditional
        techniques like Word2Vec and GloVe to cutting-edge models such as BERT and GPT, which capture the intricate
        relationships between words. These innovations not only improve text understanding but also enable the
        integration of diverse data types, including images, audio, and video, allowing AI systems to interpret
        information with greater precision. Furthermore, the rise of few-shot and zero-shot learning represents a
        significant leap toward developing adaptable AI models that can be generalized from limited examples. As we look
        ahead, it’s clear that these advancements will continue to shape the landscape of AI, fostering deeper and more
        meaningful interactions between humans and machines.
    </p>

    <br><br><br><br>

    <h1>Quiz Time !</h1>
    <div class="question">
        <p>1. Which techniques create word embeddings?</p>
        <input type="radio" name="q1" value="correct"> A) Word2Vec and GloVe<br>
        <input type="radio" name="q1" value="B"> B) Word2Vec and FastText<br>
        <input type="radio" name="q1" value="C"> C) TF-IDF and LDA<br>
        <input type="radio" name="q1" value="D"> D) K-Means and PCA<br>
    </div>
    
    <div class="question">
        <p>2. What does cosine similarity measure?</p>
        <input type="radio" name="q2" value="A"> A) The Euclidean distance between two points.<br>
        <input type="radio" name="q2" value="correct"> B) The angle between two vectors.<br>
        <input type="radio" name="q2" value="C"> C) The average of two vectors.<br>
        <input type="radio" name="q2" value="D"> D) The length of the longest vector.<br>
    </div>
    
    <button onclick="checkAnswers()">Submit Answers</button>
    
    <p id="result"></p>
    
    <script>
        function checkAnswers() {
            let correct = 0;
            let total = 2; // Total number of questions
    
            // Check answers for word embeddings
            if (document.querySelector('input[name="q1"]:checked')?.value === "correct") correct++;
            // Check answer for cosine similarity
            if (document.querySelector('input[name="q2"]:checked')?.value === "correct") correct++;
    
            document.getElementById("result").innerText = `You got ${correct} out of ${total} correct!`;
        }
    </script>
</body>

</html>
