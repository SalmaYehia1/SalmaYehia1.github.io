<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Word Embeddings</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }

        .question {
            margin: 20px 0;
        }
    </style>
</head>

<body>

    <h1>From Words to Vectors: The Journey of Word Embeddings</h1>
    <p>Have you ever wondered how AI models like Chat GPT and Gemini see words and sentences? Well, just like everything
        with AI, it’s all numbers! Words and sentences are converted to “vectors” before being passed to AI models.
        Vectors, if you’re not familiar with them, are just a list of numbers. As we shall see in this post, current AI
        models represent each word in human language by a unique list of numbers, but where it gets REALLY interesting
        is that the AI learns those numbers for every word completely on its own. You just have to give it a ton of
        data, and by data, we’re talking about a large corpus of human text, such as all of Wikipedia for example. </p>
    <h2>Types of Embeddings</h2>
    <p>This blog post will mostly focus on Word and Document embedding. However, the idea of embeddings applies to so
        much more. Embeddings, in general, are a way to convert some object, such as a word, sentence, or even an image
        into a vector. There are many types of embeddings and different approaches for each. This list is only a small
        example of the many kinds of embeddings that exist out there: </p>
    <ul>
        <li><strong>Word Embeddings</strong><br>
            Numerical representations of words capture semantic relationships, allowing computers to understand and
            process language.
        </li>
        <br>
        <li><strong>Document/Sentence Embeddings</strong><br>
            Document/sentence embeddings extend the concept of word embeddings to represent entire documents or
            sentences as single vectors. These embeddings capture the overall meaning and context of the text. These
            embeddings help in tasks like semantic search, document clustering, and text summarization.
        </li>
        <br>
        <li><strong>Image Embeddings</strong><br>
            Image embeddings are numerical representations of images, mapping them to dense vectors in a
            high-dimensional space. These vectors capture an image's features, such as color, texture, and shape.
        </li>
        <br>
        <li><strong>Graph Embeddings</strong><br>
            Graph embeddings are numerical representations of nodes in a graph, mapping them to dense vectors in a
            low-dimensional space. These vectors capture the structural and semantic relationships between nodes in the
            graph.
        </li>
    </ul>
    <h2>What are Word Embeddings?</h2>
    <p>
        Well, word embeddings mean converting a word into a vector. To be more Mathematically precise, we’re mapping, or
        “embedding”, each word to some vector in a d-dimensional vector space, where d is the dimension of the vector
        and is the same for all words. For example, if each word gets mapped to a list of 300 numbers, our word
        embeddings convert each word to a vector in a 300-dimensional vector space. But how exactly do those mappings
        work? Well, let’s dive deeper!

    </p>
    <h3>How Word Embedding Works</h3>
    <p>
        Word embedding begins by training a model on a large text dataset, like Wikipedia. First, the text is
        preprocessed to ensure the quality. This involves splitting the text into individual words, a step known as
        tokenization, and removing unnecessary elements such as stop words, punctuation, and special characters.
    </p>
    <p>
        During training, the model learns to assign a vector to each word, placing words with similar meanings closer
        together in a shared vector space. By this, word embeddings capture both the unique meaning of each word and the
        relationships between them, reflecting how they are used. This highlights the strength of word embeddings in
        positioning words that share meanings or contexts near each other, making it easier to understand the
        relationships between concepts.
    </p>
    <h3>
        Approaches to word embeddings
    </h3>
    <h4>1. Frequency-based Approaches</h4>
    <p>These approaches depend on the occurrences and the word frequency to build vector representations, such as:</p>
    <ol>
        <li><strong>One Hot Encoding</strong></li>
        <li><strong>Bag of Words (BoW)</strong></li>
        <li><strong>Co-occurrence Counts</strong></li>
        <li><strong>Term Frequency-Inverse Document Frequency (TF-IDF)</strong></li>
    </ol>

    <h4>2. Prediction-based Approaches</h4>
    <p>These approaches learn word embeddings by predicting word occurrences based on their context, such as:</p>
    <ol>
        <li><strong>Word2Vec</strong></li>
        <li><strong>GloVe (Global Vectors for Word Representation)</strong></li>
        <li><strong>BERT</strong></li>
    </ol>

    <h3>
        One Hot Encoding
    </h3>

    <p>
        This is the most basic technique to embed words, but as we shall see, it has many disadvantages. To understand
        this technique more concretely, let’s assume we will choose to set the vocabulary, that is all the words our AI
        model will ever know or ever have to deal with, to be the 50,000 most common words in the English language. One
        idea is to sort the words by any order and assign each word a number between 1 and our vocab size, which is
        50,000 in this case. Notice how we’ve already converted each word into a number? The next step is to represent
        each word by a list of numbers of length 50,000 that is all 0’s with a single 1 at the number associated with
        our target word. In the figure below, the words are sorted alphabetically, and the word Apple is assigned the
        number 1, for example. The vector associated with Apple is equal to 1 only at the first index with all the other
        values equal to 0 elsewhere. On the other hand, the last word in our vocab, Zoo, has a corresponding vector that
        is only equal to 1 at the last index and equal to 0 elsewhere.
    </p>


    <p> But why are hot encodings a bad idea? Well, there are a couple of problems with this approach:</p>
    <ol>
        <li>The Vector Dimension is too large = Vocab Size.<br></li>
        <li>The Vectors are sparse, “mostly empty”. Even though we’re using many dimensions per word, we’re
                wasting most of
                our vector dimensions by setting them to 0. We’re only making use of 1 single dimension per word when we
                set it
                equal to 1.<br></li>
        <li> All words are equally distant in our vector space. As we shall see later on, other techniques for
                word
                embeddings have an interesting property in that words with similar meanings get embedded by vectors that
                are
                closer to each other.<br></li>
    </ol>


   <h3>
      Bag Of Words (BOW) 
   </h3>
    <p>
        A very similar technique to one-hot encoding used to embed a document, instead of a single word, is known as Bag of Words. When embedding a document with a bag of words, we’re essentially getting the one-hot encoding of every word in the document and summing them all into one single vector representing the document, thus the text is defined as the frequency of words in a document while ignoring the order. The next figure shows how a bag of words would work for 3 sentences, each representing a different document, and a vocab size = 4:
    </p>
    <img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*axffCQ9ae0FHXxhuy66FbA.png" alt="Bag of Words Model">
    <p>
         
It provides a more compact representation than one-hot encoding but still lacks context and semantic meaning. Each position in the vector corresponds to a word in the vocabulary, in the order they were listed.

    </p>
    <ol>
    <li>
        Similar to one-hot encoding, the vector dimension is still too large = Vocabulary Size.
    </li>
    <li>
        Similar to one-hot encoding, the Bag of Words will still be mostly empty. While the vector in Bag of Words wouldn’t be as empty as with One-Hot-Encoded words, it will still have many unused dimensions in our vector set to 0. For a bag of words to ever have a vector that isn’t mostly empty, it needs to have a TON of words. For example, if we want more than half of our vector to not be set to 0, then we need the encoded document to have half of ALL the words in our vocabulary. For a vocab size = 50,000, we would need our document to have at least 25,000 unique words. However, this will usually not be the case for any document we’re dealing with, so we will end up with mostly empty vectors.
    </li>
    <li>
        Completely ignores the order. While this disadvantage is related to the problem of embedding documents rather than embedding a single word, it is an important problem that appears whenever we try to embed a document as the sum of embeddings for the words in that document.
    </li>
</ol>


</body>

</html>
