<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Word Embeddings</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        .question {
            margin: 20px 0;
        }
    </style>
</head>
<body>

<h1>From Words to Vectors: The Journey of Word Embeddings</h1>
<p>Have you ever wondered how AI models like Chat GPT and Gemini see words and sentences? Well, just like everything with AI, it’s all numbers! Words and sentences are converted to “vectors” before being passed to AI models. Vectors, if you’re not familiar with them, are just a list of numbers. As we shall see in this post, current AI models represent each word in human language by a unique list of numbers, but where it gets REALLY interesting is that the AI learns those numbers for every word completely on its own. You just have to give it a ton of data, and by data, we’re talking about a large corpus of human text, such as all of Wikipedia for example. </p>
<h2>Types of Embeddings</h2>
<p>This blog post will mostly focus on Word and Document embedding. However, the idea of embeddings applies to so much more. Embeddings, in general, are a way to convert some object, such as a word, sentence, or even an image into a vector. There are many types of embeddings and different approaches for each. This list is only a small example of the many kinds of embeddings that exist out there: </p>
<ul>
    <li><strong>Word Embeddings</strong><br>
        Numerical representations of words capture semantic relationships, allowing computers to understand and process language.
    </li>
    <br>
    <li><strong>Document/Sentence Embeddings</strong><br>
        Document/sentence embeddings extend the concept of word embeddings to represent entire documents or sentences as single vectors. These embeddings capture the overall meaning and context of the text. These embeddings help in tasks like semantic search, document clustering, and text summarization.
    </li>
    <br>
    <li><strong>Image Embeddings</strong><br>
        Image embeddings are numerical representations of images, mapping them to dense vectors in a high-dimensional space. These vectors capture an image's features, such as color, texture, and shape.
    </li>
    <br>
    <li><strong>Graph Embeddings</strong><br>
        Graph embeddings are numerical representations of nodes in a graph, mapping them to dense vectors in a low-dimensional space. These vectors capture the structural and semantic relationships between nodes in the graph.
    </li>
</ul>
    <h2>What are Word Embeddings?</h2>
    <p>
        Well, word embeddings mean converting a word into a vector. To be more Mathematically precise, we’re mapping, or “embedding”,  each word to some vector in a d-dimensional vector space, where d is the dimension of the vector and is the same for all words. For example, if each word gets mapped to a list of 300 numbers, our word embeddings convert each word to a vector in a 300-dimensional vector space. But how exactly do those mappings work? Well, let’s dive deeper!

    </p>
    <h3>How Word Embedding Works</h3>
    <p>
        Word embedding begins by training a model on a large text dataset, like Wikipedia. First, the text is preprocessed to ensure the quality. This involves splitting the text into individual words, a step known as tokenization, and removing unnecessary elements such as stop words, punctuation, and special characters.
    </p> 
    <p>
       During training, the model learns to assign a vector to each word, placing words with similar meanings closer together in a shared vector space. By this, word embeddings capture both the unique meaning of each word and the relationships between them, reflecting how they are used. This highlights the strength of word embeddings in positioning words that share meanings or contexts near each other, making it easier to understand the relationships between concepts.
 </p>
   <h3>
       Approaches to word embeddings
   </h3>
    <h4>1. Frequency-based Approaches</h4>
<p>These approaches depend on the occurrences and the word frequency to build vector representations, such as:</p>
<ol>
    <li><strong>One Hot Encoding</strong></li>
    <li><strong>Bag of Words (BoW)</strong></li>
    <li><strong>Co-occurrence Counts</strong></li>
    <li><strong>Term Frequency-Inverse Document Frequency (TF-IDF)</strong></li>
</ol>

<h4>2. Prediction-based Approaches</h4>
<p>These approaches learn word embeddings by predicting word occurrences based on their context, such as:</p>
<ol>
    <li><strong>Word2Vec</strong></li>
    <li><strong>GloVe (Global Vectors for Word Representation)</strong></li>
    <li><strong>BERT</strong></li>
</ol>
    
    
    


</body>
</html>
